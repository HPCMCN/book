# 1. 系统设置

这里直接搭建集群方案

## 1.1 网段划分

1. 虚拟机网段

   ```shell
   10.111.10.0/24		10.111.0.1~10.111.0.255
   
   分配方案:
   10.111.10.10	master01
   10.111.10.11	master02
   10.111.10.12	master03
   10.111.10.20	node01
   10.111.10.21	node02	
   
   vip: 10.111.10.111
   ```

2. cvs网段

   ```shell
   10.0.0.0/12		10.0.0.1~10.15.255.255
   ```

3. pod网段

   ```shell
   172.168.0.0/12		
   ```

## 1.2 标志设置

1. 根据网段的划分, 修改各个节点的名称, 方便辨识

   ```shell
   hostnamectl set-hostname master01
   hostnamectl set-hostname master02
   hostnamectl set-hostname master03
   hostnamectl set-hostname node01
   hostnamectl set-hostname node02
   
   more /etc/hostname
   
   cat >> /etc/hosts << EOF
   10.111.0.10     master01
   10.111.0.11     master02
   10.111.0.12     master03
   10.111.0.20     node01
   10.111.0.21     node02
   EOF
   ```

## 1.3 配置镜像源

1. 下载repo

   ```shell
   curl -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo
   cd /etc/yum.repos.d
   ```

2. 替换镜像源版本

   ```shell
   sed -i "s/\$releasever/7/g" CentOS-Base.repo
   sed -i -e '/mirrors.cloud.aliyuncs.com/d' -e '/mirrors.aliyuncs.com/d' /etc/yum.repos.d/CentOS-Base.repo
   ```

## 1.4 内核升级

1. 升级系统并忽略内核

   ```shell
   yum update -y --exclude=kernel* && reboot
   
   # 注意如果报错 有lib问题, 可以直接拆掉重试
   rpm -qa | grep xxx
   rpm -e xxx
   ```

2. 下载内核

   ```shell
   cd /root
   
   kernel: 链接：https://pan.baidu.com/s/1gPITwX72INeFzbqGAXQ1-Q 提取码：1111 
   kernel-dev: 链接：https://pan.baidu.com/s/1TkAPxaSzlmVdRXkEx2RnRA 提取码：1111 
   ```

3. 将下载文件同时发送到其他节点

   ```shell
   for i in master02 master03 ;do scp kernel-ml-4.19.12-1.el7.elrepo.x86_64.rpm kernel-ml-devel-4.19.12-1.el7.elrepo.x86_64.rpm $i:/root/ ; done
   ```

4. 安装内核

   ```shell
   cd /root && yum localinstall -y kernel-ml*
   ```

5. 修改内核的启动顺序

   ```shell
   grub2-set-default  0 && grub2-mkconfig -o /etc/grub2.cfg
   
   grubby --args="user_namespace.enable=1" --update-kernel="$(grubby --default-kernel)"
   
   # 检查一下是否修改
   grubby --default-kernel
   ```

6. 重启

   ```shell
   reboot
   ```

7. 检查是否生效

   ```shell
   uname -a
   ```

## 1.5 资源限制解除

1. 关闭防火墙

   ```shell
   systemctl disable --now firewalld
   systemctl disable --now dnsmasq
   systemctl disable --now NetworkManager
   ```

2. 关闭selinux资源限制

   ```shell
   setenforce 0
   sed -i 's#SELINUX=enforcing#SELINUX=disabled#g' /etc/sysconfig/selinux
   sed -i 's#SELINUX=enforcing#SELINUX=disabled#g' /etc/selinux/config
   ```

3. 网络配置

   ```shell
   sysctl net.bridge.bridge-nf-call-iptables=1
   sysctl net.bridge.bridge-nf-call-ip6tables=1
   
   cat <<EOF > /etc/sysctl.d/k8s.conf
   net.ipv4.ip_forward = 1
   net.bridge.bridge-nf-call-iptables = 1
   net.bridge.bridge-nf-call-ip6tables = 1
   fs.may_detach_mounts = 1
   vm.overcommit_memory=1
   vm.panic_on_oom=0
   fs.inotify.max_user_watches=89100
   fs.file-max=52706963
   fs.nr_open=52706963
   net.netfilter.nf_conntrack_max=2310720
   
   net.ipv4.tcp_keepalive_time = 600
   net.ipv4.tcp_keepalive_probes = 3
   net.ipv4.tcp_keepalive_intvl =15
   net.ipv4.tcp_max_tw_buckets = 36000
   net.ipv4.tcp_tw_reuse = 1
   net.ipv4.tcp_max_orphans = 327680
   net.ipv4.tcp_orphan_retries = 3
   net.ipv4.tcp_syncookies = 1
   net.ipv4.tcp_max_syn_backlog = 16384
   net.ipv4.ip_conntrack_max = 65536
   net.ipv4.tcp_max_syn_backlog = 16384
   net.ipv4.tcp_timestamps = 0
   net.core.somaxconn = 16384
   EOF
   
   
   sysctl --system
   #sysctl -p /etc/sysctl.d/k8s.conf
   ```

4. 内存限制解除

   ```shell
   swapoff -a && sysctl -w vm.swappiness=0
   sed -ri '/^[^#]*swap/s@^@#@' /etc/fstab
   ```

5. 磁盘, 文件描述符等限制解除

   ```shell
   ulimit -SHn 65535
   
   vim /etc/security/limits.conf
   # 末尾添加如下内容
   * soft nofile 65536
   * hard nofile 131072
   * soft nproc 65535
   * hard nproc 655350
   * soft memlock unlimited
   * hard memlock unlimited
   ```

## 1.6 主机时间同步

1. 下载ntpdate

   ```shell
   rpm -ivh http://mirrors.wlnmp.com/centos/wlnmp-release-centos.noarch.rpm
   yum install ntpdate -y
   ```

2. 设置本地时区

   ```shell
   ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime
   echo 'Asia/Shanghai' >/etc/timezone
   ```

3. 手动同步时间

   ```shell
   ntpdate time2.aliyun.com
   ```

4. 定时同步时间

   ```shell
   # 加入crontab 
   ​```shell
   */5 * * * * ntpdate time2.aliyun.com
   ​```
   ```

## 1.7 设置免密登录

1. 生成密钥对

   ```shell
   ssh-keygen -t rsa   # 一路回车
   ```

2. 将公钥发送到其他服务器中

   ```shell
   for i in master01 master02 master03;do ssh-copy-id -i ~/.ssh/id_rsa.pub $i;done
   ```

# 2. Haproxy+Keepalived

## 2.1 Ipvs

ipvs是LVS的核心模块

### 2.1.1 安装

```shell
yum install -y ipvs 或者ipvsadm
```

### 2.1.2 配置

```shell
vim /etc/modules-load.d/ipvs.conf 
# 添加如下信息
ip_vs
ip_vs_lc
ip_vs_wlc
ip_vs_rr
ip_vs_wrr
ip_vs_lblc
ip_vs_lblcr
ip_vs_dh
ip_vs_sh
ip_vs_fo
ip_vs_nq
ip_vs_sed
ip_vs_ftp
ip_vs_sh
nf_conntrack
ip_tables
ip_set
xt_set
ipt_set
ipt_rpfilter
ipt_REJECT
ipip
```

### 2.1.3 加载配置

```shell
# 加载到内核
systemctl enable --now systemd-modules-load.service
```

## 2.2 Haproxy

### 2.2.1 安装

```shell
yum install haproxy -y
```

### 2.2.2 配置

```shell
mkdir /etc/haproxy
vim /etc/haproxy/haproxy.cfg # 覆盖内容
global
  maxconn  2000
  ulimit-n  16384
  log  127.0.0.1 local0 err
  stats timeout 30s

defaults
  log global
  mode  http
  option  httplog
  timeout connect 5000
  timeout client  50000
  timeout server  50000
  timeout http-request 15s
  timeout http-keep-alive 15s

frontend monitor-in
  bind *:33305
  mode http
  option httplog
  monitor-uri /monitor

frontend master
  bind 0.0.0.0:16443
  bind 127.0.0.1:16443
  mode tcp
  option tcplog
  tcp-request inspect-delay 5s
  default_backend master

backend master
  mode tcp
  option tcplog
  option tcp-check
  balance roundrobin
  default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100
  server master01	10.111.0.10:6443  check
  server master02	10.111.0.11:6443  check
  server master03	10.111.0.12:6443  check
```

### 2.2.3 启动

```shell

```

## 2.3 Keepalived

### 2.3.1 安装

```shell
yum install -y keepalived
```

### 2.3.2 配置

```shell
# 分别修改/etc/keepalived/keepalived.conf文件

# master01
! Configuration File for keepalived
global_defs {
    router_id LVS_DEVEL
script_user root
    enable_script_security
}
vrrp_script chk_apiserver {
    script "/etc/keepalived/check_apiserver.sh"
   interval 5
    weight -5
    fall 2  
rise 1
}
vrrp_instance VI_1 {
    state BACKUP
    interface ens33
    mcast_src_ip 10.111.0.10
    virtual_router_id 51
    priority 100
    advert_int 2
    authentication {
        auth_type PASS
        auth_pass K8SHA_KA_AUTH
    }
    virtual_ipaddress {
        10.111.0.111
    }
    track_script {
       chk_apiserver
    }
}

#master02
! Configuration File for keepalived
global_defs {
    router_id LVS_DEVEL
script_user root
    enable_script_security
}
vrrp_script chk_apiserver {
    script "/etc/keepalived/check_apiserver.sh"
   interval 5
    weight -5
    fall 2  
rise 1
}
vrrp_instance VI_1 {
    state BACKUP
    interface ens33
    mcast_src_ip 10.111.0.11
    virtual_router_id 51
    priority 100
    advert_int 2
    authentication {
        auth_type PASS
        auth_pass K8SHA_KA_AUTH
    }
    virtual_ipaddress {
        10.111.0.111
    }
    track_script {
       chk_apiserver
    }
}


#master03
! Configuration File for keepalived
global_defs {
    router_id LVS_DEVEL
script_user root
    enable_script_security
}
vrrp_script chk_apiserver {
    script "/etc/keepalived/check_apiserver.sh"
   interval 5
    weight -5
    fall 2  
rise 1
}
vrrp_instance VI_1 {
    state BACKUP
    interface ens33
    mcast_src_ip 10.111.0.12
    virtual_router_id 51
    priority 100
    advert_int 2
    authentication {
        auth_type PASS
        auth_pass K8SHA_KA_AUTH
    }
    virtual_ipaddress {
        10.111.0.111
    }
    track_script {
       chk_apiserver
    }
}
```

### 2.3.3 设置健康检查

```shell
vim /etc/keepalived/check_apiserver.sh # 配置如下信息

#!/bin/bash

err=0
for k in $(seq 1 3)
do
    check_code=$(pgrep haproxy)
    if [[ $check_code == "" ]]; then
        err=$(expr $err + 1)
        sleep 1
        continue
    else
        err=0
        break
    fi
done

if [[ $err != "0" ]]; then
    echo "systemctl stop keepalived"
    /usr/bin/systemctl stop keepalived
    exit 1
else
    exit 0
fi
```

### 2.3.4 启动

```shell
chmod +x /etc/keepalived/check_apiserver.sh

systemctl daemon-reload
systemctl enable --now haproxy && systemctl haproxy start
systemctl enable --now keepalived && systemctl keepalived start
```

### 2.3.5 测试

```shell
 ping 10.111.0.111 -c 4
 telnet 10.111.0.111 16443  # telnet不同, 认为vip未设置成功
```

# 3. docker搭建

## 3.1 清理原有的docker

```shell
yum remove docker \
docker-client \
docker-client-latest \
docker-common \
docker-latest \
docker-latest-logrotate \
docker-logrotate \
docker-selinux \
docker-engine-selinux \
docker-engine

# 查看未拆卸的docker
rpm -qa|grep docker

# 再使用yum remove移除

rm -rf /etc/systemd/system/docker.service.d
rm -rf /var/lib/docker
rm -rf /var/run/docker
```

## 3.2 安装

1. 依赖

   ```shell
   # docker依赖
   yum install -y yum-utils   device-mapper-persistent-data   lvm2
   ```

2. 导入repo

   ```shell
   # docker repo
   yum-config-manager --add-repo  https://download.docker.com/linux/centos/docker-ce.repo
   
   yum list docker-ce --showduplicates | sort -r
   ```

3. 安装指定版本的docker

   ```shell
   yum install -y docker-ce-19.03.* docker-ce-cli-19.03.* containerd.io
   ```

## 3.3 修改k8s的配置

```shell
mkdir /etc/docker
cat > /etc/docker/daemon.json <<EOF
{
  "exec-opts": ["native.cgroupdriver=systemd"]
}
EOF
```

## 3.4 启动docker

```shell
systemctl start docker && systemctl enable docker
```

# 4. k8s集群搭建

## 4.1 kubeadm搭建k8s

### 4.1.1 安装kubeadm

1. 设置镜像源

   ```shell
   cat <<EOF > /etc/yum.repos.d/kubernetes.repo
   [kubernetes]
   name=Kubernetes
   baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
   enabled=1
   gpgcheck=1
   repo_gpgcheck=1
   gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
   EOF
   ```

2. 安装

   ```shell
   # 依赖
   yum install -y wget jq psmisc vim net-tools telnet yum-utils device-mapper-persistent-data lvm2 git 
   
   
   yum list kubeadm.x86_64 --showduplicates | sort -r
   
   yum install kubeadm-1.20* kubelet-1.20* kubectl-1.20* -y
   ```

3. 配置kubelet

   ```shell
   # 此操作包含两个步骤: 1. 配置diver, 与docker保持一致, 2. 配置镜像源
   cat >/etc/sysconfig/kubelet<<EOF
   KUBELET_EXTRA_ARGS="--cgroup-driver=systemd --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.2"
   EOF
   ```

4. 刷新网络配置

   ```shell
   systemctl daemon-reload && systemctl daemon-reload
   ```

### 4.1.2 kubeadm安装k8s

1. 配置文件

   ```shell
   # vim kubernetes.yaml
   
   apiVersion: kubeadm.k8s.io/v1beta2
   bootstrapTokens:
   - groups:
     - system:bootstrappers:kubeadm:default-node-token
     token: 7t2weq.bjbawausm0jaxury
     ttl: 24h0m0s
     usages:
     - signing
     - authentication
   kind: InitConfiguration
   localAPIEndpoint:
     advertiseAddress: 10.111.0.10
     bindPort: 6443
   nodeRegistration:
     criSocket: /var/run/dockershim.sock
     name: master01
     taints:
     - effect: NoSchedule
       key: node-role.kubernetes.io/master
   ---
   apiServer:
     certSANs:
     - 10.111.0.111
     timeoutForControlPlane: 4m0s
   apiVersion: kubeadm.k8s.io/v1beta2
   certificatesDir: /etc/kubernetes/pki
   clusterName: kubernetes
   controlPlaneEndpoint: 10.111.0.111:16443
   controllerManager: {}
   dns:
     type: CoreDNS
   etcd:
     local:
       dataDir: /var/lib/etcd
   imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers
   kind: ClusterConfiguration
   kubernetesVersion: v1.20.0
   networking:
     dnsDomain: cluster.local
     podSubnet: 172.168.0.0/12
     serviceSubnet: 10.0.0.0/12
   scheduler: {}
      
   ```

2. 更新语法

   ```shell
    # 为防止此语法, 或者版本老旧问题, 使用自带命令进行更新, 将新文件同步到其他机器上
   kubeadm config migrate --old-config kubernetes.yaml --new-config new.yaml
   ```

3. 拉取镜像

   ```shell
   kubeadm config images pull --config /root/new.yaml 
   # 如果拉取失败可以直接使用docker拉取
   # vim images.sh
   # -----------------------------------
   #!/bin/bash
   url=registry.cn-hangzhou.aliyuncs.com/google_containers
   version=v1.20.0
   images=(`kubeadm config images list --kubernetes-version=$version|awk -F '/' '{print $2}'`)
   for imagename in ${images[@]} ; do
     docker pull $url/$imagename
     #docker tag $url/$imagename k8s.gcr.io/$imagename
     #docker rmi -f $url/$imagename
   done
   # -----------------------------------
   # 注意如果失败, 需要重置后才能重新初始化
   kubeadm reset -f ; ipvsadm --clear  ; rm -rf ~/.kube
   ```

4. 初始化环境

   ```shell
   kubeadm init --config /root/new.yaml  --upload-certs
   
   # 输出结果将会有两种join
   1.  kubeadm join xx --token xx
       --discovery-token-ca-cert-hash xx
       --control-plane --certificate-key xx
       
   # 此节点为master节点加入操作
   2. kubeadm join 10.111.0.111:16443 --token xxx
       --discovery-token-ca-cert-hash xxx
   # 此节点为node加入操作
   ```

5. 添加master或者node

   ```shell
   # 将主节点的输出结果复制到node/master执行即可, 如果过期, 则需要如下操作
   
   # 1. 生成token
   kubeadm token create --print-join-command
   
   # 2. 生成master的--certificate-key
   kubeadm init phase upload-certs  --upload-certs
   ```

6. 配置master的环境变量

   ```shell
   cat <<EOF >> /root/.bashrc
   export KUBECONFIG=/etc/kubernetes/admin.conf
   EOF
   source /root/.bashrc
   ```

7. 检查各个节点的状态

   ```shell
   kubectl get nodes
   kubectl get pods -n kube-system -o wide
   ```

### 4.1.3 kubectl安装Calico

1. 下载资源

   ```shell
   链接：https://pan.baidu.com/s/1FhM8CHa3mILZfamLeA5nHQ 
   提取码：1111 
   
   cd /root/k8s-ha-install && git checkout manual-installation-v1.20.x && cd calico/
   ```

2. 修改master节点信息

   ```shell
   sed -i 's#etcd_endpoints: "http://<ETCD_IP>:<ETCD_PORT>"#etcd_endpoints: "https://10.111.0.10:2379,https://10.111.0.12:2379"#g' calico-etcd.yaml
   
   # vim calico-etcd.yaml  修改内容信息:
   # etcd_endpoints: "https://10.111.0.10:2379,https://10.111.0.12:2379"
   ```

3. 修改etcd证书位置

   ```shell
   ETCD_CA=`cat /etc/kubernetes/pki/etcd/ca.crt | base64 | tr -d '\n'`
   ETCD_CERT=`cat /etc/kubernetes/pki/etcd/server.crt | base64 | tr -d '\n'`
   ETCD_KEY=`cat /etc/kubernetes/pki/etcd/server.key | base64 | tr -d '\n'`
   sed -i "s@# etcd-key: null@etcd-key: ${ETCD_KEY}@g; s@# etcd-cert: null@etcd-cert: ${ETCD_CERT}@g; s@# etcd-ca: null@etcd-ca: ${ETCD_CA}@g" calico-etcd.yaml
   ```

4. 修改pod证书位置

   ```shell
   sed -i 's#etcd_ca: ""#etcd_ca: "/calico-secrets/etcd-ca"#g; s#etcd_cert: ""#etcd_cert: "/calico-secrets/etcd-cert"#g; s#etcd_key: "" #etcd_key: "/calico-secrets/etcd-key" #g' calico-etcd.yaml
   ```

5. 修改pod网段

   ```shell
   POD_SUBNET=`cat /etc/kubernetes/manifests/kube-controller-manager.yaml | grep cluster-cidr= | awk -F= '{print $NF}'`
   
   # 检查一下
   # 搜索CALICO_IPV4POOL_CIDR, 解开注释, 并设置为pod的网段
   -name: CALICO_IPV4POOL_CIDR
    value: "172.168.0.0/16"
   ```

6. 安装

   ```shell
   kubectl apply -f calico-etcd.yaml
   ```

### 4.1.4 kubectl安装metrics

1. 将证书发往其他节点

   ```shell
   for i in master02 master03 ;do scp /etc/kubernetes/pki/front-proxy-ca.crt $i:/etc/kubernetes/pki/front-proxy-ca.crt ; done
   ```

2. 安装metrics

   ```shell
   cd /root/k8s-ha-install/metrics-server-0.4.x-kubeadm/
   
   kubectl  create -f comp.yaml
   ```

3. 查看状态

   ```shell
   kubectl get pods -n kube-system -o wide  # 等待metrics-server状态变为running
   kubectl  top node
   ```

### 4.1.5 安装DashBoard

1. 安装dashboard

   ```shell
   cd /root/k8s-ha-install/dashboard
   kubectl  create -f .
   ```

2. 配置浏览器

   ```shell
   # 在路径中添加: --test-type --ignore-certificate-errors
   ```

   ![image-20210803095839358](file://E:\work\00-总结\.img\05-k8s搭建\image-20210803095839358.png?lastModify=1628043555)

3. 修改svc为转发端口

   ```shell
   kubectl edit svc kubernetes-dashboard -n kubernetes-dashboard
   # 修改type为NodePort
   ```

4. 查看dashboard暴露的端口

   ```shell
   kubectl get svc kubernetes-dashboard -n kubernetes-dashboard
   ```

5. 访问页面

   ```shell
   https://10.111.0.111:31755  # ip为vip
   ```

6. 登录token获取

   ```shell
   kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}')
   ```

### 4.1.6 将HA应用到集群

1. 查看当前模式

   ```shell
   curl 127.0.0.1:10249/proxyMode
   ```

2. 修改配置

   ```shell
   kubectl edit cm kube-proxy -n kube-system
   修改信息 mode: "ipvs"
   ```

3. 更新pod

   ```shell
   kubectl patch daemonset kube-proxy -p "{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"date\":\"`date +'%s'`\"}}}}}" -n kube-system
   ```

4. 重新检查pod

   ```shell
   curl 127.0.0.1:10249/proxyMode
   ```

### 4.1.7 移除污点

默认k8s不允许非系统自带服务安装到master节点中, 所以想要在master中安装三方服务, 需要取消污点

1. 查看污点

   ```shell
   kubectl  describe node -l node-role.kubernetes.io/master=  | grep Taints
   ```

2. 删除污点

   ```shell
   kubectl  taint node  -l node-role.kubernetes.io/master node-role.kubernetes.io/master:NoSchedule-
   ```

## 4.2 二进制搭建