# 安装

官网: https://github.com/chatchat-space/Langchain-Chatchat

环境要求:

* python >= 3.10

* 独显 >= 10G

* CUDA Toolkit: https://developer.nvidia.com/cuda-12-1-0-download-archive


1. 下载 Langchain-Chatchat

   ```shell
   git clone https://github.com/chatchat-space/Langchain-Chatchat
   ```

2. 安装依赖

   ```shell
   # workspace: Langchain-Chatchat
   pip install -r requirements.txt -r requirements_api.txt -r requirements_webui.txt
   ```

3. 重置config文件

   ```shell
   # 注意此操作只需要执行一次, 否则配置修改的内容将会被重置掉
   python copy_config_example.py
   ```

4. 初始化数据

   ```shell
   python init_database.py --recreate-vs
   ```

5. 下载模型

   ```shell
   # workspace: Langchain-Chatchat
   git lfs install
   
   # #### 注意需要vpn(本机需要启动代理程序) ####
   # git config --global http.proxy socks5://127.0.0.1:10808
   # git config --global https.proxy socks5://127.0.0.1:10808
   
   # #### 取消代理 ####
   # git config --global --unset http.proxy
   # git config --global --unset https.proxy
   
   # 模型下载特别慢
   git clone https://huggingface.co/THUDM/chatglm3-6b  # 大概25G左右
   git clone https://huggingface.co/BAAI/bge-large-zh  # 大概3G左右
   ```

6. 修改配置

   ```shell
   # workspace: Langchain-Chatchat/configs
   # vim model_config.py
   
   # 按照流程的话, 就不许要修改, 否则修改你自己的模型
   # 注意, 一次只能运行一个模型, 多个模型写入进去, 在使用时, 也会消耗时间去实时导入的
   LLM_MODELS = ["chatglm3-6b", "zhipu-api", "openai-api"]
   
   # 模型导入位置, workspace: Langchain-Chatchat
   # 比如chatglm3-6b模型, 就相当于: Langchain-Chatchat/chatglm3-6b为github clone下来的模型
   MODEL_ROOT_PATH = ""
   
   
   # vim server_config.py
   # 服务器地址
   DEFAULT_BIND_HOST = "0.0.0.0"
   
   # 电脑太垃圾了, 限制一下吧
   "max_gpu_memory": "8GiB", # 每个GPU占用的最大显存
   "load_8bit": True, # 开启8bit量化
   ```

7. 开始启动服务

   ```shell
   python startup.py -a
   ```

   按照提示, 输入一个邮箱号后, 开始正式启动服务

   ![image-20240403151419037](.image/01-%E5%AE%89%E8%A3%85/image-20240403151419037.png)

